## 1 项目简介

这篇论文是由我和实验室的博士师兄一起完成的，他提供了论文的思路，并且完成了论文的写作，我主要完成论文实验部分。

这篇论文的主题是神经网络架构搜索。具体一点就是如何针对资源受限的计算设备搜索出精度和速度相对平衡的神经网络架构。目前的神经网络在各个 AI 领域都有应用，但是一个普遍存在的是问题是对算力的要求过高。通常将大型神经网络放在服务器上运行，服务器将端侧传来的数据进行神经网络推理预测，然后将结果传回端侧。这样的方式会导致数据的安全性问题也会因为数据传输导致时延的增加。目前已经有将神经网络部署在端侧的方案，比如网络架构的量化，轻量型网络架构的设计等。量化是通过网络结构里面的参数由浮点数量化为低比特的整数，比如 8bit 的 int 型。来将浮点数运算变成了整数运算，加速运算。显然，该方式会降低精度。类似于 mobilenet 这种轻量型网络架构的设计方案依靠架构设计里面的 trick。比如深度可分离卷积，通道融合等。这种方式需要大量的经验而且效果往往不一定好。我们想到的方式是我们让边缘计算设备参与网络架构的设计过程，就是实时地去感知算力的消耗和精度。从尝试的各个架构中选出最好的那个架构。也就是网络架构搜索。具体的方案是，先设计一个网络架构的搜索空间，比如网络层数的范围，具体每个层的参数范围。定义好输入输出的格式。相当于我设定了很多条线路，可以选择一条线路来走，再评估每条线路的好坏，也就是算力消耗，精度。设定好网络架构搜索空间后，就是搜索策略的设定。我们将网络精度的评估和参数的训练放在服务器上，然后将服务器上生成的网络架构实时地传递到计算设备上，我们这里选用的是英伟达的 jetson nano 作为低算力设备。这个设备有一个 arm 架构 cpu，然后有 gpu 计算单元。总体功耗大概 5 瓦。cpu 算力可能和树莓派差不多，图形处理能力应该比一般的手机要强一点。是英伟达专门设计的边缘计算设备。这个 nano 板收到参数后，会根据参数生成网络架构，然后用该架构在一张随机生成的图片做推理。我们会计算推理时延作为算力消耗的评判标准。我们一般让它推理 10 次取平均值消除偶然因素。最后将推理得到的时延回传给服务器做训练。

服务器这边会根据时延参数和精度实时调整架构和架构参数，最后将精度和时延平衡得最好得架构保存。这就是整个项目得大致流程。

我们主要的工作内容是搭建了一个 NAS 框架。该框架可以针对精度，时延，内存等各个目标做网络架构搜索。

然后我们设计了一个较为合理的网络搜索空间。

## 2 项目重点和难点

项目中的重点当然在于搜索策略上的实现和

网络搜索空间的设计

网络搜索空间基于 mobilenetv2 ，mobilenetv2 是由一个个的 bottleneck 组成的，每个bottleneck 可以选择 不同的卷积核大小核膨胀率。分别是 3*3 5 * 5 7 * 7，还有 3 倍，6 倍膨胀率。当然还有 zerolayer这种选择。就是 7 种选择。除开最前面的预处理卷积层核最后的特征融合层和全连接层。中间一种由 20 个 bottleneck。一种有 7 的 20次方相当于 8 * 10 的 16 次方这么多可能性。也就是搜索空间的大小。

搜索策略

我们将离散的网络空间架构给连续化，否则就不用应用梯度下降策略来做反向传播了。将每个bottleck 设置称一个 mixed-edge，即 0 到 7 种可能性每个都设置一个具体的值。然后我们选取其中最大的一个值当作网络的架构。我们在训练过程种要训练这个值的参数。然后再推理阶段将值最大的那个选出来推理。为了训练更加合理，我们加入了一个 warm_up 过程。就是预训练搜索空间。而不进行架构采集。这样可以保证架构采集的时候精度不至于过低。最后我们的目标设定为一个表达式，即 Acc * latency/L 当latency 大于 L 的时候表现为奖励，低于的时候表现为惩罚。通过计算目标值与真实值的差值来反向优化网络架构和网络参数。



## 3 项目中遇到的问题和解决方案

**边缘计算设备和服务器之间是如何通信的**

通过 socket 进行通信的，python 封装了 socket，可以很方便地调用 socket 接口进行通信。发送的是 json 文件，用来标记网络架构。网络参数不同传输过去，因为精度信息在服务器这边可以获得，且更快。传过去的网络架构参数重建网络以后进行随机初始化就好，算力的消耗只和网络具体架构有关，与网络中每个参数没有关系。然后用随机初始化的网络在一张随机生成的图片上进行网络推理获取时延并回传时延到服务器上。

两个印象较为深刻的 bug，一是线程设置导致的 bug 问题。pytorch 框架是支持多线程训练的，当时设定得太高了，运行过程是没问题的，只是会抖动。但是在 debug 过程中会突然变慢，然后整个机器没有任何反应，只能重启，然后重新开始。跟师兄讲了以后，师兄告诉我去看 linux 的系统日志，然后我发现是内存不足，才反应过来是线程数量过高，后来改为 32 个线程，刚好是 cpu 的核心数量，问题解决了。

二是网络架构参数生成时的一个 bug，代码中有一个类，该类就是网络架构空间，叫做 supernet，类里面我定义了一个方法，目的是将某个时刻网络架构空间的参数给采集生成一个 json 文件然后再发给 nano 板。当然，这个架构参数在不停的变化。所以不同的时刻该方法生成的架构参数是不一样的。但是在训练过程我发现时延一直没有变化，然后花了一周多时间定位到架构参数采集时发生了问题。采集的参数时没有变化的。当时用了 beyond compare 这个软件将中间过程的参数挨个进行对比才发现问题。问题的原因有点复杂，是 python 类运行机制的问题，我现在有点忘了。我是通过新建一个搜索空间类，然后将每次训练过程中的变化的地方作用于这个新建的搜索空间类然后这个类转么用来生成网络架构参数。问题才解决的。



